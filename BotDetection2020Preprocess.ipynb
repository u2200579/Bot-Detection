{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BotDetection2020Preprocess.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMDtxWVIwXiUd2ui6ll3IpA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/u2200579/Bot-Detection/blob/main/BotDetection2020Preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aI_TTQ8CYWpk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efb2cd4e-2a28-411e-bd2d-75a9112bbb55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install vaderSentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIcjmLAzw0QW",
        "outputId": "07e80ea7-2a9c-4f74-8c05-fae59adedbe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYKdigjBRLgx",
        "outputId": "591fd2f0-9eeb-4b17-801d-2f35853f1c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.0.0.tar.gz (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 4.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.0.0-py3-none-any.whl size=193022 sha256=22154dee31bf0b63de0d80af3cbbe3a1783152391d07c2c471f16e74c047f5a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/29/4d/3cfe7452ac7d8d83b1930f8a6205c3c9649b24e80f9029fc38\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import emoji\n",
        "import json\n",
        "from textblob import TextBlob\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "import nltk  \n",
        "nltk.download('punkt')\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re"
      ],
      "metadata": {
        "id": "NGWkMdHD61bx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f22137f-d75b-49c3-e55b-936b2f555277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/My Drive/train.json\" \"train.json\"\n",
        "!cp \"/content/drive/My Drive/dev.json\" \"dev.json\"\n",
        "!cp \"/content/drive/My Drive/test.json\" \"test.json\""
      ],
      "metadata": {
        "id": "YG6JEEYk649P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json(\"test.json\")#read json file\n",
        "json_struct=json.loads(df['profile'].to_json(orient='records'))\n",
        "dfs1 = pd.DataFrame(json_struct) #flatten nested dictionary of profile into its own dataframe\n",
        "dfs1 = dfs1.rename(columns={\"id\": \"ID\"})#rename dataframe column "
      ],
      "metadata": {
        "id": "Y3ZBIh6VeQKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sid_obj = SentimentIntensityAnalyzer() #variable for sentiment calculator\n",
        "def subject_calc(text): #function to calculate subjectivity of tweets\n",
        "    try:\n",
        "        return TextBlob(text).sentiment.subjectivity \n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def sentiment_calc(text): #function to calculate sentiment of tweets\n",
        "    try:\n",
        "        return  sid_obj.polarity_scores(text)\n",
        "    except:\n",
        "        return 0\n",
        "        \n",
        "def mean_embeddings(s):\n",
        "    \"\"\"Transfer a list of words into mean embedding\"\"\"\n",
        "    return np.mean([glove_vec.get_vector(x) for x in s if x in glove_vec], axis=0) #function to extract word embedding for words in tweets"
      ],
      "metadata": {
        "id": "26BOrlQyFxd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_vec = gensim.downloader.load('glove-twitter-25')#load in word embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTc9jvCIrjYc",
        "outputId": "0f67fad6-a1fa-44f1-9579-eb253e06fa0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2=df.explode('tweet') #create a row for each tweet \n",
        "df3 = df2.drop([\"profile\", 'neighbor','domain',\t'label'], axis=1) #Remove other columns beside ID \n",
        "df2['tweet'] = df2['tweet'].fillna('').apply(str) #convert tweet to string value\n",
        "df3['mentions'] = df3['tweet'].str.count(\"@\") #count the number of @'s contained in each tweet\n",
        "df3['hashtags'] = df3['tweet'].str.count(\"#\") #count the number of #\n",
        "df3['urls'] = df3['tweet'].str.count(\"https\") #count the number of URL links\n",
        "df3['retweets'] = df3['tweet'].str.count(\"RT\") #count the number of RT in each tweet\n",
        "df3['subjectivity_score'] = df3['tweet'].apply(subject_calc) #Apply function calculating subjectivity of an tweet to each tweet \n",
        "df3['sentiment_score'] = df3['tweet'].apply(sentiment_calc) #Apply function calculating sentiment scores of an tweet to each tweet \n",
        "df1 = pd.concat([df3.drop(['sentiment_score'], axis=1), df3['sentiment_score'].apply(pd.Series)], axis=1) #create new dataframe splitting dictionary of sentiment scores of each tweet into own column to be used as features"
      ],
      "metadata": {
        "id": "vGq7qRn4-ygS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df1.groupby(\"ID\").mean().reset_index()#to find average of each tweet content and semantic information for each user"
      ],
      "metadata": {
        "id": "JuOdrulqvm_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import operator #used to build vocabulary to check similarity between word embeddings and tweet tokens\n",
        "\n",
        "def check_coverage(vocab,embeddings_index):\n",
        "    a = {}\n",
        "    oov = {}\n",
        "    k = 0\n",
        "    i = 0\n",
        "    for word in tqdm(vocab):\n",
        "        try:\n",
        "            a[word] = embeddings_index[word]\n",
        "            k += vocab[word]\n",
        "        except:\n",
        "\n",
        "            oov[word] = vocab[word]\n",
        "            i += vocab[word]\n",
        "            pass\n",
        "\n",
        "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
        "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
        "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
        "\n",
        "    return sorted_x\n",
        "\n",
        "def build_vocab(sentences, verbose =  True):\n",
        "    \"\"\"\n",
        "    :param sentences: list of list of words\n",
        "    :return: dictionary of words and their count\n",
        "    \"\"\"\n",
        "    vocab = {}\n",
        "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
        "        for word in sentence:\n",
        "            try:\n",
        "                vocab[word] += 1\n",
        "            except KeyError:\n",
        "                vocab[word] = 1\n",
        "    return vocab\n",
        "  \n",
        "sentences = df2['tweet'].progress_apply(lambda x: x.split()).values\n",
        "vocab = build_vocab(sentences)\n",
        "print({k: vocab[k] for k in list(vocab)[:5]})"
      ],
      "metadata": {
        "id": "nCem1LdJssRj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a424baf6-f969-46c5-c5d8-af610fff538b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 199863/199863 [00:01<00:00, 185562.66it/s]\n",
            "100%|██████████| 199863/199863 [00:01<00:00, 119877.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'RT': 66265, '@clevelanddotcom:': 1, 'Three': 101, 'Ohio': 65, 'House': 721}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing (tweet):\n",
        "  #Remove RT\n",
        "  tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "  #  Replace hyperlinks with URL\n",
        "  tweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', ' url ', tweet2)\n",
        "  #  Replace hashtags with 'hashtag' \n",
        "  tweet2 = re.sub(r'#', ' hashtag ', tweet2)\n",
        "  # Replace @User with user\n",
        "  tweet2 = re.sub(r'@[A-Za-z0-9]+', ' user ', tweet2)\n",
        "  #Remove \\n from text\n",
        "  tweet2 = re.sub(r'\\n', '', tweet2)\n",
        "  #Replace date values with date\n",
        "  tweet2 = re.sub(r'(\\d+/\\d+/\\d+)', 'date', tweet2)\n",
        "  #Remove the \n",
        "  tweet2 = re.sub(r'_', ' ', tweet2)\n",
        "  tweet2 = re.sub(r'-', ' ', tweet2)\n",
        "  # Replace numeric terms in the tweet with 'number'.\n",
        "  tweet2 = re.sub(r'/^[+-]?((\\d+(\\.\\d*)?)|(\\.\\d+))$/', ' number ', tweet2)\n",
        "  return (tweet2)\n",
        "\n",
        "\n",
        "def _get_mispell(mispell_dict):\n",
        "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
        "    return mispell_dict, mispell_re\n",
        "\n",
        "\n",
        "mispell_dict = {\"i’m\":\"i am\",\n",
        "                \"isn’t\":\"is not\",\n",
        "                \"it’s\":\"it is\",\n",
        "                \"don’t\":\"do not\",\n",
        "                \"can’t\":\"can not\",\n",
        "                \"we're\":\"we are\",\n",
        "                \"that's\":\"that is\",\n",
        "                \"i've\":\"i have\",\n",
        "                \"you’re\":\"you are\",\n",
        "                \"he’s\":\"he is\",\n",
        "                \"couldn't\":\"could not\",\n",
        "                \"wouldn't\":\"would not\",\n",
        "                \"shouldn't\":\"should not\",\n",
        "                \"ain't\":\"are not\",\n",
        "                }\n",
        "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
        "\n",
        "def replace_typical_misspell(text):\n",
        "    def replace(match):\n",
        "        return mispellings[match.group(0)]\n",
        "\n",
        "    return mispellings_re.sub(replace, text)\n",
        "\n",
        "def clean_numbers(x):\n",
        "    #replace number from text with #\n",
        "    x = re.sub('[0-9]{5,}', '#####', x)\n",
        "    x = re.sub('[0-9]{4}', '####', x)\n",
        "    x = re.sub('[0-9]{3}', '###', x)\n",
        "    x = re.sub('[0-9]{2}', '##', x)\n",
        "    x = re.sub('[0-9]', '#', x)\n",
        "    return x\n",
        "\n",
        "def clean_text(x):\n",
        "   #remove punctuation from text\n",
        "    x = str(x)\n",
        "    for punct in \"/-'\":\n",
        "        x = x.replace(punct, ' ')\n",
        "    for punct in '&':\n",
        "        x = x.replace(punct, f' {punct} ')\n",
        "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
        "        x = x.replace(punct, '')\n",
        "    return x\n",
        "\n",
        "df2['tweet'] = df2['tweet'].apply(lambda x: emoji.demojize(x, delimiters=(\" \", \" \"))) #apply emoji transformer\n",
        "df2['tweet'] = df2['tweet'].progress_apply(lambda x: replace_typical_misspell(x)) #apply preprocessing concatenated words into seperate words\n",
        "df2['tweet'] = df2['tweet'].apply(preprocessing) #apply preprocessing\n",
        "df2['tweet'] = df2['tweet'].apply(lambda x: clean_text(x))\n",
        "df2['tweet'] = df2['tweet'].progress_apply(lambda x: clean_numbers(x))  #apply preprocessing to transform numbers\n",
        "df2['tweets'] = df2['tweet'].apply(lambda x:x.lower()) #lower text\n",
        "df2['tweets'] = df2['tweets'].progress_apply(lambda x: word_tokenize(x)) #tokenise text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5uEDQ4VIj3p",
        "outputId": "51885d9d-8789-4aa9-dd9d-5513e9532549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 199863/199863 [00:04<00:00, 45509.52it/s]\n",
            "100%|██████████| 199863/199863 [00:01<00:00, 149808.11it/s]\n",
            "100%|██████████| 199863/199863 [00:35<00:00, 5605.60it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "used to change text as close as possible to word embeddings https://www.kaggle.com/code/christofhenkel/how-to-preprocessing-when-using-embeddings/notebook"
      ],
      "metadata": {
        "id": "jQVvdJXzbb7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = build_vocab(df2['tweets']) \n",
        "oov = check_coverage(vocab,glove_vec) # compares all of tweets with words in word embedding "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNDw1oFW0OsP",
        "outputId": "1bc6b4bb-626d-4c4f-adc6-36c97acc982b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 199863/199863 [00:01<00:00, 147283.56it/s]\n",
            "100%|██████████| 200653/200653 [00:00<00:00, 262968.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found embeddings for 51.87% of vocab\n",
            "Found embeddings for  95.46% of all text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "oov[:10]#provides list of words that are in tweets and not in word embedding"
      ],
      "metadata": {
        "id": "VqPBsoZKsfB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2[\"Text_Sim\"] = df2['tweets'].progress_apply(lambda x: mean_embeddings(x))\n",
        "h = []\n",
        "df2 = df2.dropna(subset=['Text_Sim'])\n",
        "s = df2.groupby(\"ID\")\n",
        "h = s['Text_Sim'].apply(np.stack).apply(cosine_similarity).apply(np.mean).reset_index()\n",
        "# extract 'embeddings' for each group\n",
        "#  .apply(np.stack) # turns sequence of arrays into proper matrix\n",
        "#  .apply(cosine_similarity) # compute pairwise similarity matrix\n",
        "#  .apply(np.mean) # get the mean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEY-1fZ6L3Bs",
        "outputId": "9c75e765-5d70-4a0a-f638-227d01b1e036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 4702/199863 [00:00<00:16, 12003.06it/s]/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "100%|██████████| 199863/199863 [00:17<00:00, 11665.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = df[['ID', 'label']]\n",
        "dfs1['ID'] = dfs1['ID'].astype(\"float\").astype(\"Int64\")\n",
        "df1 = pd.merge(df1,h,on=[\"ID\"])\n",
        "df1 = pd.merge(dfs1,df1,on=[\"ID\"])\n",
        "new_df = pd.merge(df1,new_df,on=[\"ID\"])\n",
        "new_df = new_df.dropna(axis=1, how='all')\n",
        "new_df #create new dataframe containing preprocessed tweet content and semantic features and combine with dataframe containing user profile and labels for account"
      ],
      "metadata": {
        "id": "W7dePY1k3d4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.to_csv('/content/drive/My Drive/testset.csv',encoding='utf-8',index=False) #upload new dataframe into csv file for modeling"
      ],
      "metadata": {
        "id": "bLIjtZ35sxPm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}