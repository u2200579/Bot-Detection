{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19737,"status":"ok","timestamp":1662063894005,"user":{"displayName":"Topé .O","userId":"04571267219552919109"},"user_tz":-60},"id":"GQSYwjM8jqMk","outputId":"37392fc2-4016-4e1d-87e2-da45bbfefb87"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4738,"status":"ok","timestamp":1661823743046,"user":{"displayName":"Topé .O","userId":"04571267219552919109"},"user_tz":-60},"id":"HKZpzoaaFnPo","outputId":"6b4feb3b-1f55-4772-8ebd-e7f5974a1511"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting vaderSentiment\n","  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 13.8 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2022.6.15)\n","Installing collected packages: vaderSentiment\n","Successfully installed vaderSentiment-3.3.2\n"]}],"source":["pip install vaderSentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4890,"status":"ok","timestamp":1661823747927,"user":{"displayName":"Topé .O","userId":"04571267219552919109"},"user_tz":-60},"id":"AfZACMGlFsvN","outputId":"a1585e22-dbe1-41f8-ee38-9eb0eab7a741"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting emoji\n","  Downloading emoji-2.0.0.tar.gz (197 kB)\n","\u001b[K     |████████████████████████████████| 197 kB 15.6 MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-2.0.0-py3-none-any.whl size=193022 sha256=82cc8bc1f633f777c87ea462905af7f51dc3ff9953a952bd0de57b79c2303b4f\n","  Stored in directory: /root/.cache/pip/wheels/ec/29/4d/3cfe7452ac7d8d83b1930f8a6205c3c9649b24e80f9029fc38\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-2.0.0\n"]}],"source":["pip install emoji"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3548,"status":"ok","timestamp":1662063936464,"user":{"displayName":"Topé .O","userId":"04571267219552919109"},"user_tz":-60},"id":"2XZLh-aGr3l0","outputId":"6d24aa34-4663-4a80-ce25-4e3e3720e173"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import emoji\n","from textblob import TextBlob\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n","from sklearn.metrics.pairwise import cosine_similarity\n","import gensim\n","import gensim.downloader\n","import nltk  \n","nltk.download('punkt')\n","from tqdm import tqdm\n","tqdm.pandas()\n","from nltk.tokenize import word_tokenize\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WGz8YR3rnLVs"},"outputs":[],"source":["!cp \"/content/drive/My Drive/genuineusers.csv\" \"genuineusers.csv\" #Import user Profile information for geneuine users and bots\n","!cp \"/content/drive/My Drive/SS1users.csv\" \"SS1users.csv\"\n","!cp \"/content/drive/My Drive/SS2users.csv\" \"SS2users.csv\" #Files were too large for Tabula submission\n","!cp \"/content/drive/My Drive/SS3users.csv\" \"SS3users.csv\"\n","!cp \"/content/drive/My Drive/TS1users.csv\" \"TS1users.csv\""]},{"cell_type":"code","source":["!cp \"/content/drive/My Drive/genuinetweets.csv\" \"genuinetweets.csv\"\n","!cp \"/content/drive/My Drive/SS1tweets.csv\" \"SS1tweets.csv\"\n","!cp \"/content/drive/My Drive/SS2tweets.csv\" \"SS2tweets.csv\" #Files were too large for Tabula submission\n","!cp \"/content/drive/My Drive/SS3tweets.csv\" \"SS3tweets.csv\"\n","!cp \"/content/drive/My Drive/TS1tweets.csv\" \"TS1tweets.csv\" #Import Tweet information for geneuine users and bots of social spambot group and traditional spambot 1"],"metadata":{"id":"-oOJr9Q1P6U1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"maJyDdZVqFM9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662065535447,"user_tz":-60,"elapsed":1920,"user":{"displayName":"Topé .O","userId":"04571267219552919109"}},"outputId":"26e87fb7-c970-41f4-cce0-5ec151ce12a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fake users done!\n","Human users done!\n"]}],"source":["dfs = []\n","\n","dfs.append(pd.read_csv(\"SS1users.csv\",parse_dates=['created_at']))\n","dfs.append(pd.read_csv(\"SS2users.csv\",parse_dates=['created_at']))\n","dfs.append(pd.read_csv(\"SS3users.csv\",parse_dates=['created_at']))\n","dfs1 = pd.read_csv(\"TS1users.csv\",parse_dates=['created_at'])\n","dfs1['created_at'] = dfs1['created_at'].str.replace(r'\\D+', '', regex=True).astype('int')\n","dfs1['created_at'] = pd.to_datetime(dfs1['created_at'],unit='ms')\n","dfs1['is_bot'] = 1\n","dfs1['Data'] = 'TraditionalSocialSpambot'\n","\n","botsCsv = pd.concat(dfs, axis=0, sort=True)\n","botsCsv['is_bot'] = 1\n","botsCsv['Data'] = 'SocialSpambot'\n","print(\"Fake users done!\")\n","dfs2 = []\n","dfs2.append(pd.read_csv(\"genuineusers.csv\",parse_dates=['created_at']))\n","\n","print(\"Human users done!\")\n","\n","humansCsv = pd.concat(dfs2, sort=True)\n","humansCsv['is_bot']=0\n","humansCsv['Data']='Human'\n","\n","resultUsers = pd.concat([botsCsv, dfs1, humansCsv], sort=True)\n","resultUsers['created_at'] = pd.to_datetime(resultUsers['created_at'],utc=True)\n","resultUsers['crawled_at'] = pd.to_datetime(resultUsers['crawled_at'],utc=True)\n","users = pd.DataFrame()\n","users['id']=resultUsers['id']\n","users['status_count'] = resultUsers['statuses_count']\n","users['friends_count'] = resultUsers['friends_count']\n","users['favorites_count'] = resultUsers['favourites_count']\n","users['followers_count'] = resultUsers['followers_count']\n","users['friends_to_followers_ratio'] = resultUsers['friends_count']/resultUsers['followers_count']\n","users['listed_count'] = resultUsers['listed_count']\n","users['is_bot'] = resultUsers['is_bot']\n","users['Account_age'] = (resultUsers['crawled_at'] - resultUsers['created_at']).dt.days\n","users['Data'] = resultUsers['Data']\n","users['description'] = resultUsers['description']\n","users['screen_name'] = resultUsers['screen_name']\n","users['name'] = resultUsers['name']\n","users['default_profile'] = resultUsers['default_profile']\n","users['profile_use_background_image'] = resultUsers['profile_use_background_image']\n"]},{"cell_type":"code","source":["df2 = []\n","dfs3 = []\n","\n","df2.append(pd.read_csv(\"SS1tweets.csv\", encoding='ISO-8859-1'))\n","df2.append(pd.read_csv(\"SS2tweets.csv\", encoding='ISO-8859-1'))\n","df2.append(pd.read_csv(\"SS3tweets.csv\", encoding='ISO-8859-1'))\n","df2.append(pd.read_csv(\"TS1tweets.csv\", encoding='ISO-8859-1'))\n","\n","botsTweets = pd.concat(df2, sort=True)\n","botsTweets['is_bot'] = 1\n","\n","\n","print(\"Fake users done!\")\n","dfs2 = []\n","dfs2.append(pd.read_csv(\"genuinetweets.csv\", encoding='ISO-8859-1'))\n","\n","print(\"Human users done!\")\n","\n","humansTweets = pd.concat(dfs2, sort=True)\n","humansTweets['is_bot']=0\n","humansTweets = humansTweets.drop([2839361]) #datasets of bots and genuine accounts (drop a duplicate)\n","df = pd.concat([botsTweets, humansTweets], sort=True)"],"metadata":{"id":"wN4fxrcKP4zi"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0_hQfFN8D53u"},"outputs":[],"source":["df1 = df[['user_id','num_hashtags','num_urls','num_mentions','retweet_count','is_bot']] #create dataframe containing tweet content information\n","df2 = df[['user_id','text']] #create dataframe for tweets to calculate semantic analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uw6fJh4UwTjB"},"outputs":[],"source":["sid_obj = SentimentIntensityAnalyzer() #variable for sentiment calculator\n","def subject_calc(text): #function to calculate subjectivity of tweets\n","    try:\n","        return TextBlob(text).sentiment.subjectivity \n","    except:\n","        return 0\n","\n","def sentiment_calc(text): #function to calculate sentiment of tweets\n","    try:\n","        return  sid_obj.polarity_scores(text)\n","    except:\n","        return 0\n","        \n","def mean_embeddings(s):\n","    \"\"\"Transfer a list of words into mean embedding\"\"\"\n","    return np.mean([glove_vec.get_vector(x) for x in s if x in glove_vec], axis=0) #function to extract word embedding for words in tweetsd embedding for words in tweets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aVpAjNVBTj2y"},"outputs":[],"source":["df2['subjectivity_score'] = df2['text'].apply(subject_calc) #Apply function calculating subjectivity of an tweet to each tweet \n","df2['sentiment_score'] = df2['text'].apply(sentiment_calc) #Apply function calculating sentiment scores of an tweet to each tweet\n","df2 = pd.concat([df2.drop(['sentiment_score'], axis=1), df2['sentiment_score'].apply(pd.Series)], axis=1) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":62648,"status":"ok","timestamp":1661823884374,"user":{"displayName":"Topé .O","userId":"04571267219552919109"},"user_tz":-60},"id":"0_0_LY6NTzQO","outputId":"72aa986a-d537-4d45-aa39-fe9e18f64568"},"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 104.8/104.8MB downloaded\n"]}],"source":["glove_vec = gensim.downloader.load('glove-twitter-25') #upload glove embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MUoe1nodRrGj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661823884375,"user_tz":-60,"elapsed":38,"user":{"displayName":"Topé .O","userId":"04571267219552919109"}},"outputId":"8202a6da-b836-474f-834a-765b65c4aae6"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 5/5 [00:00<00:00, 6026.30it/s]\n","100%|██████████| 5/5 [00:00<00:00, 28455.25it/s]"]},{"output_type":"stream","name":"stdout","text":["{'I': 1, 'Pooh': 1, '-': 2, 'In': 1, 'silenzio': 1}\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import operator  #used to build vocabulary to check similarity between word embeddings and tweet tokens\n","\n","\n","def check_coverage(vocab,embeddings_index):\n","    a = {}\n","    oov = {}\n","    k = 0\n","    i = 0\n","    for word in tqdm(vocab):\n","        try:\n","            a[word] = embeddings_index[word]\n","            k += vocab[word]\n","        except:\n","\n","            oov[word] = vocab[word]\n","            i += vocab[word]\n","            pass\n","\n","    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n","    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n","    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n","\n","    return sorted_x\n","\n","def build_vocab(sentences, verbose =  True):\n","    \"\"\"\n","    :param sentences: list of list of words\n","    :return: dictionary of words and their count\n","    \"\"\"\n","    vocab = {}\n","    for sentence in tqdm(sentences, disable = (not verbose)):\n","        for word in sentence:\n","            try:\n","                vocab[word] += 1\n","            except KeyError:\n","                vocab[word] = 1\n","    return vocab\n","df2['text']=df2['text'].fillna('').apply(str)\n","sentences = df2['text'].progress_apply(lambda x: x.split()).values\n","vocab = build_vocab(sentences)\n","print({k: vocab[k] for k in list(vocab)[:5]})"]},{"cell_type":"markdown","source":["used to change text as close as possible to word embeddings https://www.kaggle.com/code/christofhenkel/how-to-preprocessing-when-using-embeddings/notebook\n","\n"],"metadata":{"id":"XaGNGcPQLaUt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"F5uEDQ4VIj3p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661824444386,"user_tz":-60,"elapsed":314,"user":{"displayName":"Topé .O","userId":"04571267219552919109"}},"outputId":"fdf59a26-a04b-443c-d737-444df82efd95"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 5/5 [00:00<00:00, 10300.35it/s]\n","100%|██████████| 5/5 [00:00<00:00, 10496.26it/s]\n","100%|██████████| 5/5 [00:00<00:00, 3685.68it/s]\n"]}],"source":["def preprocessing (tweet):\n","  #Remove RT\n","  tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n","  #  Replace hyperlinks with URL\n","  tweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', ' url ', tweet2)\n","  #  Replace hashtags with 'hashtag' \n","  tweet2 = re.sub(r'#', ' hashtag ', tweet2)\n","  # Replace @User with user\n","  tweet2 = re.sub(r'@[A-Za-z0-9]+', ' user ', tweet2)\n","  #Remove \\n from text\n","  tweet2 = re.sub(r'\\n', '', tweet2)\n","  #Replace date values with date\n","  tweet2 = re.sub(r'(\\d+/\\d+/\\d+)', 'date', tweet2)\n","  #Remove the connectors for the emoji's\n","  tweet2 = re.sub(r'_', ' ', tweet2)\n","  tweet2 = re.sub(r'-', ' ', tweet2)\n","  # Replace numeric terms in the tweet with 'number'.\n","  tweet2 = re.sub(r'/^[+-]?((\\d+(\\.\\d*)?)|(\\.\\d+))$/', ' number ', tweet2)\n","  return (tweet2)\n","\n","\n","def _get_mispell(mispell_dict):\n","    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n","    return mispell_dict, mispell_re\n","\n","\n","mispell_dict = {\"i’m\":\"i am\",\n","                \"isn’t\":\"is not\",\n","                \"it’s\":\"it is\",\n","                \"don’t\":\"do not\",\n","                \"can’t\":\"can not\",\n","                \"we're\":\"we are\",\n","                \"that's\":\"that is\",\n","                \"i've\":\"i have\",\n","                \"you’re\":\"you are\",\n","                \"he’s\":\"he is\",\n","                \"couldn't\":\"could not\",\n","                \"wouldn't\":\"would not\",\n","                \"shouldn't\":\"should not\",\n","                \"ain't\":\"are not\",\n","                }\n","mispellings, mispellings_re = _get_mispell(mispell_dict)\n","\n","def replace_typical_misspell(text):\n","    def replace(match):\n","        return mispellings[match.group(0)]\n","\n","    return mispellings_re.sub(replace, text)\n","\n","def clean_numbers(x):\n","    #replace number from text with #\n","    x = re.sub('[0-9]{5,}', '#####', x)\n","    x = re.sub('[0-9]{4}', '####', x)\n","    x = re.sub('[0-9]{3}', '###', x)\n","    x = re.sub('[0-9]{2}', '##', x)\n","    x = re.sub('[0-9]', '#', x)\n","    return x\n","\n","def clean_text(x):\n","   #remove punctuation from text\n","    x = str(x)\n","    for punct in \"/-'\":\n","        x = x.replace(punct, ' ')\n","    for punct in '&':\n","        x = x.replace(punct, f' {punct} ')\n","    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n","        x = x.replace(punct, '')\n","    return x\n","df2['text'] = df2['text'].fillna('').apply(str) \n","df2['text'] = df2['text'].apply(lambda x: emoji.demojize(x, delimiters=(\" \", \" \")))#apply emoji transformer\n","df2['text'] = df2['text'].progress_apply(lambda x: replace_typical_misspell(x))#apply preprocessing concatenated words into seperate words\n","df2['text'] = df2['text'].apply(preprocessing)#apply preprocessing\n","df2['text'] = df2['text'].apply(lambda x: clean_text(x))\n","df2['text'] = df2['text'].progress_apply(lambda x: clean_numbers(x)) #apply preprocessing to transform numbers\n","df2['text'] = df2['text'].apply(lambda x:x.lower())#lower text\n","df2['text'] = df2['text'].progress_apply(lambda x: word_tokenize(x)) #tokenise text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jz5DbBcdR0uE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661823884376,"user_tz":-60,"elapsed":23,"user":{"displayName":"Topé .O","userId":"04571267219552919109"}},"outputId":"bb40c041-fced-4d40-c490-63a42a2f8f94"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 5/5 [00:00<00:00, 30393.51it/s]\n","100%|██████████| 17/17 [00:00<00:00, 37468.82it/s]"]},{"output_type":"stream","name":"stdout","text":["Found embeddings for 94.12% of vocab\n","Found embeddings for  95.83% of all text\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["vocab = build_vocab(df2['text']) \n","oov = check_coverage(vocab,glove_vec) # compares all of tweets with words in word embedding "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_0SPDlgSR72f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661823884376,"user_tz":-60,"elapsed":18,"user":{"displayName":"Topé .O","userId":"04571267219552919109"}},"outputId":"ac13095d-fea3-43f6-cc79-6d5b17e1ff52"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('baccalã', 1)]"]},"metadata":{},"execution_count":17}],"source":["oov[:20]#provides list of words that are in tweets and not in word embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a6y3hCgW_nWc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661824458943,"user_tz":-60,"elapsed":292,"user":{"displayName":"Topé .O","userId":"04571267219552919109"}},"outputId":"34b67151-55d0-4236-a16a-6132d19747a4"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 5/5 [00:00<00:00, 2069.83it/s]\n"]}],"source":["df2[\"Text_Sim\"] = df2['text'].progress_apply(lambda x: mean_embeddings(x)) # extract 'embeddings' for each group\n","h = []\n","df2 = df2.dropna(subset=['Text_Sim']) #drop rows that have NAN values\n","s = df2.groupby(\"user_id\")\n","h = s['Text_Sim'].apply(np.stack).apply(cosine_similarity).apply(np.mean).reset_index()\n","#  .apply(np.stack) # turns sequence of arrays into proper matrix\n","#  .apply(cosine_similarity) # compute pairwise similarity matrix\n","#  .apply(np.mean) # get the mean"]},{"cell_type":"code","source":["df1=df1.groupby(\"user_id\").mean().reset_index() # calculate average of Tweet content information\n","df2=df2.groupby(\"user_id\").mean().reset_index() # calculate average of semantic analysis of Tweet information"],"metadata":{"id":"vxq6R0VzBhwk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#concat users profile info, processed Tweet Content, and Tweet Semantic Analysis\n","df1 = pd.merge(h,df1,on=[\"user_id\"])\n","df1 = pd.merge(df2,df1,on=[\"user_id\"])\n","df1 = df1.rename(columns={\"user_id\": \"id\"})#rename dataframe column \n","df1['id'] = df1['id'].astype(\"float\").astype(\"Int64\")\n","df1 = df1.drop(columns=['is_bot'])\n","users = users.merge(df1,how='left', left_on=\"id\", right_on=\"id\")#merge so no information is lost"],"metadata":{"id":"rU2z5TruXc6R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["users # should be concated dataframe of all features"],"metadata":{"id":"3Yz8NZf4IqKM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FZKPJdQDM8br"},"outputs":[],"source":["df = users\n","df.to_csv('/content/drive/My Drive/preprocess.csv',encoding='utf-8',index=False) #create processed dataframe for machine learning model"]}],"metadata":{"colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyMIO9uzJrucGW3XrWRlAUL7"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}